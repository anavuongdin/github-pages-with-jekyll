---
title: "Bài 6: Giới thiệu về bài toán điều khiển (Control)"
date: 2022-02-14
image: /assets/img/reinforcement-learning.webp
---

Như vậy, ta đã làm quen với một bài toán lớn trong học tăng cường là bài toán dự đoán giá trị, bây giờ, ta sẽ xem xét một bài toán lớn còn lại trong học tăng cường, đó là bài toán điều khiển. Bài toán điều khiển có mục tiêu là học chính sách tối ưu $\pi^{\ast}$. Trong bài toán Multi-armed bandits, thực chất, ta cũng đã giải quyết bài toán này; các thuật toán mà blog đã trình bày bao gồm $\epsilon$-greedy và gradient bandit. Ta sẽ cùng nhau phân tích kỹ hơn về bài toán điều khiển trong học tăng cường.

# Các dạng bài toán học trong học tăng cường
Trong học tăng cường, các phương pháp học thường được chia thành 2 loại chính, đó là:
- Có tương tác: Trong phương pháp học này được chia làm hai loại: 
  - Online learning.
  - Active learning.
- Không tương tác: Trong quá trình học, tác tử không tương tác thêm với môi trường.


# Testing out latex

$$ \nabla_\boldsymbol{x} J(\boldsymbol{x}) $$

# Tesing out image
{:refdef: style="text-align: center;"}
  ![Minh họa về Học tăng cường]({{ page.image | relative_url }}){: .center-image }  
  *Hình 1: Minh họa về Học tăng cường* 
{: refdef}


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
