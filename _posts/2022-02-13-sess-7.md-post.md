---
title: "Bài 7: Thuật toán Q-Learning"
date: 2022-02-13
image1: /assets/img/post7/q-learning.PNG
---

Ta sẽ tìm hiểu về các phương pháp đưa ra lời giải bằng cách tìm ra hàm hành động-giá trị tối ưu $Q^{\ast}$. Ưu điểm của hàm hành động-giá trị so với hàm giá trị là nó chứa thêm thông tin về đánh giá các hành động, vì vậy, có thể vừa tận dụng để đưa ra chính sách. Ý tưởng là nếu chúng ta có $Q_k$ gần với $Q^{\ast}$ thì chính sách tham lam theo $Q_k$ sẽ gần với chính sách tối ưu. Thuật toán đầu tiên được giới thiệu trong bài viết này là thuật toán Q-Learning.

## Thuật toán Q-Learning
Ta xét một bộ chuyển vị, bao gồm: $(X_t, A_t, R_t, Y_{t+1})$. Ta xét các đại lượng sau:

$$ \delta_{t+1}(Q) = R_{t+1}+\gamma\max_{b\in\mathcal{A}}Q(Y_{t+1}, b)-Q(X_t, A_t)$$

$$ Q_{t+1}(x, a) = Q_t(x, a) +\alpha_t\delta_{t+1}(Q)\mathbb{I_{x=X_t,a=A_t}}, \forall x, a\in\mathcal{X}\times\mathcal{A}$$

Ở bộ chuyển vị trên, ta xét $Y_{t+1}$ thay vì $X_{t+1}$ để tổng quát hóa. Bạn đọc còn nhớ về khái niệm on-policy và off-policy, nếu như $Y_{t+1}\neq X_{t+1}$, nghĩa là sự chuyển vị trạng thái trong quá trình học khác với quá trình đưa ra hành động thì được gọi là off-policy. Ngược lại, nếu chúng ta sử dụng học online trong quá trình cập nhật từng bước, ta có: $X_{t+1}=Y_{t+1}$ và tất nhiên nó được gọi là on-policy. Chú ý rằng, **Q-Learning là off-policy**. Lý do bởi đại lượng:

$$ \max_{b\in\mathcal{A}}Q(Y_{t+1}, b)$$

Được tính toán dựa trên so sánh toàn bộ các hành động $b$ nên trạng thái $Y_{t+1}$ phải được sinh từ một tập mẫu. Ở bài sau, ta sẽ được so sánh với một thuật toán on-policy để bạn đọc có thể hình dung thêm về vấn đề này.

Thuật toán này có nét tương đồng với TD(0) và cũng được lý thuyết chứng minh rằng sẽ hội tụ tới $Q^{\ast}$ trong một số điều kiện nhất định, dưới đây là mã giả cập nhật thuật toán Q-Learning:

# Tesing out image
{:refdef: style="text-align: center;"}
  ![Thuật toán Q-Learning]({{ page.image1 | relative_url }}){: .center-image }  
  *Hình 1: Thuật toán Q-Learning* 
{: refdef}


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
