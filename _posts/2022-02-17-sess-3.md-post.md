---
title: "Bài 3: Bài toán Multi-armed bandits"
date: 2022-02-17
image: /assets/img/post3/slot-machine.png
---

Đầu tiên, ta sẽ xem xét một bài toán được gọi là "Hello world" đối với lĩnh vực học tăng cường, đó là bài toán "Multi-armed bandits". Ta xét môi trường trong trường hợp đơn giản nhất, đó là môi trường chỉ có duy nhất một trạng thái, và bất cứ hành động nào thực hiện với môi trường cũng làm môi trường không thay đổi trạng thái. Đối với môi trường như vậy, thì ta chỉ quan tâm tới tập hành động; mục tiêu chính của ta là tìm xem hành động nào đem lại tổng phần thưởng mong đợi lớn nhất. Đây cũng chính là bài toán "Multi-armed bandits". Trong bài toán này, tác tử là chúng ta, môi trường là một không gian (giả sử là casino) bao gồm các máy đánh bạc (slot machine). Ta cần phải có chiến thuật sao cho phần thưởng ta thu được thì quán casino này là lớn nhất.

{:refdef: style="text-align: center;"}
  ![Minh họa về Máy đánh bạc]({{ page.image | relative_url }}){: .center-image }  
  *Hình 1: Minh họa về Máy đánh bạc* 
{: refdef}

Trên hình là minh họa về một Máy đánh bạc, là một thành phần của môi trường mà ta tương tác. Máy đánh bạc đơn giản chỉ có 1 tay kéo (pull arm), mỗi lần chơi, ta đơn giản chỉ cần kéo cái tay đó và sẽ nhận được phần thưởng theo một luật cụ thể. Trong phạm vi bài viết này, ta không quan tâm tới luật đó, mà ta chỉ quan tâm tới các yếu tố toán học mô hình bài toán này. Ta giả sử rằng, phần thưởng mà ta nhận được từ mỗi lần kéo là tuân theo một phân phối xác suất nào đó. Ta sẽ mô hình hóa cụ thể bài toán dưới đây.

Bây giờ tưởng tượng ta có $k$ máy đánh bạc tại casino. Không gian hành động của chúng ta là $\mathcal{A} = \{ 1,2,\dots, k\}$, trong đó, hành động $i$ thể hiện rằng ta sẽ tới máy đánh bạc thứ $i$ và thực hiện kéo cái tay của máy đó. Mỗi máy thứ $i$ sẽ có một phân phối xác suất về phần thưởng $R{\sim}P_i$ khác nhau. Ví dụ, ta có thể hiểu $P_i = \mathcal{N}(\mu_i,\,\sigma_i^{2})$, tuy nhiên thực tế, các phân phối xác suất này có thể không phải chỉ là phân phối chuẩn. Tại mỗi bước $t=0,1,\dots$, ta chọn hành động $A_t\in\mathcal{A}$ sẽ thu được phần thưởng $R_t$ sinh ra từ phân phối xác suất $P_{A_t}$. Nhiệm vụ của chúng ta là phải có một chính sách để có thể thu được tổng phần thưởng kỳ vọng tối đa.

## Phân tích bài toán
Ta thấy rằng: $r(x, a) = \mathbb{E}[R_{(x, a)}]$, và vì không gian trạng thái của chúng ta chỉ có duy nhất một trạng thái nên $x$ cố định, nên thực tế:

$$r(x, a) = \mathbb{E}[R_{(x, a)}] = \mathbb{E}[P_a]$$

Vậy, hiển nhiên rằng, nếu $\mathbb{E}[P_{a^{\ast}}] = \max_{a\in\mathcal{A}}\mathbb{E}[P_a]$ thì ta chỉ việc luôn chọn $a^{\ast}$ để luôn thu được tổng phần thưởng kỳ vọng tối đa. 

Như vậy, phải chăng bài toán này chỉ đơn giản là tìm kỳ vọng của các xác suất đầu vào $P_i$ rồi so sánh chọn kỳ vọng lớn nhất? Không, bài toán trong học tăng cường không đơn giản như vậy. Như đã đề cập phần trước, $P_i$ thậm chí không phải là những phân phối xác suất mà chúng ta biết và thực tế là ta cũng khó có thể mô hình hóa được mà chỉ tính toán ước lượng qua các phương pháp xác suất thống kê. Hơn nữa, kể cả môi trường của chúng ta xác định được $P_i$ nhưng không có nghĩa tác tử của chúng ta biết được $P_i$, nó phải học phân phối này qua các tương tác với môi trường. Tới đây, có hai khái niệm quan trọng của học tăng cường được xem xét và cũng đã đề cập ở bài đầu tiên, đó là khai phá (*exploration*) và tận dụng (*exploitation*).

Ta sẽ nói về tận dụng trước, tận dụng đơn giản là dựa vào những tri thức mà ta biết về môi trường để đưa ra được quyết định tốt nhất. Trong ngữ cảnh này, chính là việc nếu ta ước lượng được các $\widetilde{P_i}\approx P_i$ thì ta sẽ chọn hành động $a^{\ast} = \text{argmax}_{a\in\mathcal{A}} \mathbb{E}[\widetilde{P_a}]$ (như đã phân tích ở trước).

Tiếp theo, ta sẽ nói đến khai phá, một câu hỏi tưởng chừng là đơn giản nhưng không dễ để trả lời, đó là làm thế nào để chúng ta biết được máy đánh bạc nào sẽ đưa ra phần thưởng lớn nhất? Rõ ràng, nếu chỉ chơi 1 lần ở hai máy $C$ và $D$, giả sử tổng phần thưởng thu được ở $C$ là $15$, ở $D$ là $10$, ta so sánh $15>10$ rồi vội vàng đi đến kết luận máy $C$ sẽ đem lại phần thưởng tốt hơn máy $D$. Cách làm như vậy là chưa đủ căn cứ, lý do bởi phần thưởng được sinh ra bởi một phân phối xác suất. Trong một lần sinh xác suất bởi hai phân phối $P_C, P_D$ thì kể cả trong trường hợp kỳ vọng của $P_C$ có lớn hơn $P_D$ nhưng vẫn có thể xảy ra trường hợp giá trị sinh ra bởi $P_C$ thấp hơn $P_D$. Như vậy, bên cạnh tận dụng, ta cần phải có một chiến thuật để khai phá không gian trạng thái, hành động.

Cả tận dụng và khai phá đều là hai khái niệm quan trọng trong học tăng cường, và có thể nói là chúng trade-off lẫn nhau. Một giải thuật học tăng cường luôn phải đảm bảo 2 yếu tố này và phải xem xét kỹ lưỡng trong quá trình thiết kế giải thuật.



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
