---
title: "Bài 3: Bài toán Multi-armed bandits"
date: 2022-02-17
image: /assets/img/post3/slot-machine.png
---

Đầu tiên, ta sẽ xem xét một bài toán được gọi là "Hello world" đối với lĩnh vực học tăng cường, đó là bài toán "Multi-armed bandits". Ta xét môi trường trong trường hợp đơn giản nhất, đó là môi trường chỉ có duy nhất một trạng thái, và bất cứ hành động nào thực hiện với môi trường cũng làm môi trường không thay đổi trạng thái. Đối với môi trường như vậy, thì ta chỉ quan tâm tới tập hành động; mục tiêu chính của ta là tìm xem hành động nào đem lại tổng phần thưởng mong đợi lớn nhất. Đây cũng chính là bài toán "Multi-armed bandits". Trong bài toán này, tác tử là chúng ta, môi trường là một không gian (giả sử là casino) bao gồm các máy đánh bạc (slot machine). Ta cần phải có chiến thuật sao cho phần thưởng ta thu được thì quán casino này là lớn nhất.

{:refdef: style="text-align: center;"}
  ![Minh họa về Máy đánh bạc]({{ page.image | relative_url }}){: .center-image }  
  *Hình 1: Minh họa về Máy đánh bạc* 
{: refdef}

Trên hình là minh họa về một Máy đánh bạc, là một thành phần của môi trường mà ta tương tác. Máy đánh bạc đơn giản chỉ có 1 tay kéo (pull arm), mỗi lần chơi, ta đơn giản chỉ cần kéo cái tay đó và sẽ nhận được phần thưởng theo một luật cụ thể. Trong phạm vi bài viết này, ta không quan tâm tới luật đó, mà ta chỉ quan tâm tới các yếu tố toán học mô hình bài toán này. Ta giả sử rằng, phần thưởng mà ta nhận được từ mỗi lần kéo là tuân theo một phân phối xác suất nào đó. Ta sẽ mô hình hóa cụ thể bài toán dưới đây.

Bây giờ tưởng tượng ta có $k$ máy đánh bạc tại casino. Không gian hành động của chúng ta là $\mathcal{A} = \{ 1,2,\dots, k\}$, trong đó, hành động $i$ thể hiện rằng ta sẽ tới máy đánh bạc thứ $i$ và thực hiện kéo cái tay của máy đó. Mỗi máy thứ $i$ sẽ có một phân phối xác suất về phần thưởng $R{\sim}P_i$ khác nhau. Ví dụ, ta có thể hiểu $P_i = \mathcal{N}(\mu_i,\,\sigma_i^{2})$, tuy nhiên thực tế, các phân phối xác suất này có thể không phải chỉ là phân phối chuẩn. Tại mỗi bước $t=0,1,\dots$, ta chọn hành động $A_t\in\mathcal{A}$ sẽ thu được phần thưởng $R_t$ sinh ra từ phân phối xác suất $P_{A_t}$. Nhiệm vụ của chúng ta là phải có một chính sách để có thể thu được tổng phần thưởng kỳ vọng tối đa.

## Phân tích bài toán
Ta thấy rằng: $r(x, a) = \mathbb{E}[R_{(x, a)}]$, và vì không gian trạng thái của chúng ta chỉ có duy nhất một trạng thái nên $x$ cố định, nên thực tế:

$$r(x, a) = \mathbb{E}[R_{(x, a)}] = \mathbb{E}[P_a]$$

Vậy, hiển nhiên rằng, nếu $\mathbb{E}[P_{a^{\ast}}] = \max_{a\in\mathcal{A}}\mathbb{E}[P_a]$ thì ta chỉ việc luôn chọn $a^{\ast}$ để luôn thu được tổng phần thưởng kỳ vọng tối đa. 

Như vậy, phải chăng bài toán này chỉ đơn giản là tìm kỳ vọng của các xác suất đầu vào $P_i$ rồi so sánh chọn kỳ vọng lớn nhất? Không, bài toán trong học tăng cường không đơn giản như vậy. Như đã đề cập phần trước, $P_i$ thậm chí không phải là những phân phối xác suất mà chúng ta biết và thực tế là ta cũng khó có thể mô hình hóa được mà chỉ tính toán ước lượng qua các phương pháp xác suất thống kê. Hơn nữa, kể cả môi trường của chúng ta xác định được $P_i$ nhưng không có nghĩa tác tử của chúng ta biết được $P_i$, nó phải học phân phối này qua các tương tác với môi trường. Tới đây, có hai khái niệm quan trọng của học tăng cường được xem xét và cũng đã đề cập ở bài đầu tiên, đó là khai phá (*exploration*) và tận dụng (*exploitation*).

Ta sẽ nói về tận dụng trước, tận dụng đơn giản là dựa vào những tri thức mà ta biết về môi trường để đưa ra được quyết định tốt nhất. Trong ngữ cảnh này, chính là việc nếu ta ước lượng được các $\widetilde{P_i}\approx P_i$ thì ta sẽ chọn hành động $a^{\ast} = \text{argmax}_{a\in\mathcal{A}} \mathbb{E}[\widetilde{P_a}]$ (như đã phân tích ở trước).

Tiếp theo, ta sẽ nói đến khai phá, một câu hỏi tưởng chừng là đơn giản nhưng không dễ để trả lời, đó là làm thế nào để chúng ta biết được máy đánh bạc nào sẽ đưa ra phần thưởng lớn nhất? Rõ ràng, nếu chỉ chơi 1 lần ở hai máy $C$ và $D$, giả sử tổng phần thưởng thu được ở $C$ là $15$, ở $D$ là $10$, ta so sánh $15>10$ rồi vội vàng đi đến kết luận máy $C$ sẽ đem lại phần thưởng tốt hơn máy $D$. Cách làm như vậy là chưa đủ căn cứ, lý do bởi phần thưởng được sinh ra bởi một phân phối xác suất. Trong một lần sinh xác suất bởi hai phân phối $P_C, P_D$ thì kể cả trong trường hợp kỳ vọng của $P_C$ có lớn hơn $P_D$ nhưng vẫn có thể xảy ra trường hợp giá trị sinh ra bởi $P_C$ thấp hơn $P_D$. Như vậy, bên cạnh tận dụng, ta cần phải có một chiến thuật để khai phá không gian trạng thái, hành động.

Cả tận dụng và khai phá đều là hai khái niệm quan trọng trong học tăng cường, và có thể nói là chúng trade-off lẫn nhau. Một giải thuật học tăng cường luôn phải đảm bảo 2 yếu tố này và phải xem xét kỹ lưỡng trong quá trình thiết kế giải thuật.

## Tạo một test case cho bài toán
Đây là phần đầu tiên trong toàn bộ blog sẽ trình bày chi tiết về lập trình một bài toán Học tăng cường cũng như một phương pháp để giải bài toán. Đầu tiên, chúng ta cần khởi tạo không gian về trạng thái, hành động của bài toán (vì trạng thái là bất biến trong bài toán Multi-armed bandits nên ta chỉ quan tâm tới các hành động)
{% highlight python %}
class Arm:
  def __init__(self, probability: float, deviation: float):
    self.probability = probability
    self.deviation = deviation
    self.positive_reward = 1
    self.negative_reward = 0
  
  def pull(self) -> int:
    return self.positive_reward * np.random.normal(self.probability, self.deviation)
{% endhighlight %}

Xét một trường hợp đơn giản, khi ta có: $P_i=\mathcal{N}(\mu_i,\,\sigma_i^{2})$. Lớp Arm sẽ trừu tượng hóa các máy đánh bạc bằng cách lưu các thông tin về phân phối xác suất; ngoài ra, lớp này cung cấp một phương thức pull để có thể nhận được phần thưởng. Tiếp theo, ta định nghĩa về quán casino qua một lớp là Fair, lớp này là tập hợp của các máy đánh bạc:

{% highlight python %}
class Fair:
  def __init__(self):
    self.arms = []
    self.length = 0
  
  def add_arm(self, arm: Arm):
    self.arms.append(arm)
    self.length += 1
  
  def pull_at_machine(self, index: int) -> int:
    try:
      return self.arms[index].pull()
    except:
      print("Error at Fair.pull_at_machine")
      raise
{% endhighlight %}

Cuối cùng, ta tạo ví dụ một bài toán cụ thể như sau:
{% highlight python %}
f = Fair()
f.add_arm(Arm(0.7, 0.01))
f.add_arm(Arm(0.8, 0.01))
f.add_arm(Arm(0.6, 0.01))
f.add_arm(Arm(0.5, 0.03))
f.add_arm(Arm(0.7, 0.02))
f.add_arm(Arm(0.7, 0.01))
f.add_arm(Arm(0.8, 0.01))
f.add_arm(Arm(0.6, 0.03))
f.add_arm(Arm(0.5, 0.05))
f.add_arm(Arm(0.7, 0.02))
f.add_arm(Arm(0.7, 0.03))
f.add_arm(Arm(0.8, 0.06))
f.add_arm(Arm(0.6, 0.01))
f.add_arm(Arm(0.5, 0.01))
f.add_arm(Arm(0.9, 0.01))
f.add_arm(Arm(0.7, 0.01))
f.add_arm(Arm(0.91, 0.01))
f.add_arm(Arm(0.6, 0.02))
f.add_arm(Arm(0.5, 0.04))
f.add_arm(Arm(0.7, 0.05))
{% endhighlight %}

Như vậy, ta đặt chú ý nhất vào arm thứ 17, arm mà có giá trị kỳ vọng là 0.91, đây cũng sẽ là phân phối xác suất mà ta cần tìm ra.

## Thuật toán epsilon-greedy
Bây giờ, ta sẽ xem xét một giải thuật cũng được coi là "Hello world" đối với học tăng cường, đó là thuật toán $\epsilon$-greedy. Giải thuật này rất đơn giản. Tại mỗi thời điểm $t$, ta chỉ cần tính hai đại lượng sau: $Q_t(a)$ - đại lượng thể hiện phần thưởng trung bình nếu hành động $a$ được chọn trước thời điểm $t$ và $A_t$ là hành động chọn tại thời điểm $t$.

Tiếp theo, ta sẽ tận dụng bằng cách chọn tham lam $A_t = \text{argmax}_{a\in\mathcal{A}} Q_t(a)$. Ta sẽ cân bằng việc tận dụng này bằng chiến lược khai phá như sau: Cố định $\epsilon$ là một ngưỡng. Tại mỗi thời điểm, ta sẽ lựa chọn ngẫu nhiên một hành động với xác suất $\epsilon$, và chọn tham lam với xác suất $1-\epsilon$. Ví dụ, ta chọn $\epsilon=0.01$, khi đó mã nguồn của ta như sau:

{% highlight python %}
class Agent:
  def __init__(self, fair: Fair):
    self.reward_at_machine = [0 for i in range(fair.length)]
    self.trial_times = [0 for i in range(fair.length)]
    self.fair = fair
    self.epsilon = 0.01
    self.action_length = fair.length
  
  def choose_action(self):
    if random.random() < self.epsilon:
      machine = random.randint(0, self.action_length - 1)
    else:
      machine = self.reward_at_machine.index(max(self.reward_at_machine))

    self.trial_times[machine] += 1
    reward = self.fair.pull_at_machine(machine)
    self.reward_at_machine[machine] = (self.reward_at_machine[machine] * 
                                       (self.trial_times[machine]-1)+reward)/ \
                                       self.trial_times[machine]
    return reward

  def drive(self, num_episodes: int, num_timesteps_per_episode: int):
    rewards = []
    for episode in range(num_episodes):
      reward = 0
      for timestep in range(num_timesteps_per_episode):
        reward += self.choose_action()
      
      rewards.append(reward / num_timesteps_per_episode)
    
    return rewards
{% endhighlight %}

Trong mã nguồn trên $\texttt{self.reward_at_machine}$ chính là hàm $Q_t(a)$ mà ta đề cập, người đọc có thể dễ dàng kiểm chứng công thức cập nhật của $Q_t(a)$.
 
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
