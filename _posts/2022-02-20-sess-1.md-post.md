---
title: "Bài 1: Giới thiệu về Reinforcement Learning (Học tăng cường)"
date: 2022-02-20
image: /assets/img/reinforcement-learning.webp
---

Trong bài viết này, mình sẽ trình bày các khái niệm về Reinforcement Learning. Vì mình là một người thích dùng tiếng Việt khi trình bày về các kiến thức nên mình sẽ thống nhất tên gọi mình sẽ dùng trong suốt blog là Học tăng cường.

# Giới thiệu về bài toán trong học tăng cường
Nhắc tới học tăng cường, ta đang nhắc tới cả một bài toán trong lĩnh vực học máy và cả một mảng nghiên cứu rất lâu đời của học máy.

{:refdef: style="text-align: center;"}
  ![Minh họa về Học tăng cường]({{ page.image | relative_url }}){: .center-image }  
  *Hình 1: Minh họa về Học tăng cường* 
{: refdef}

Một bài toán trong học tăng cường thường có mô thức được minh họa ở hình 1. Trong bài toán này, chúng ta có một tác tử (*agent*) tương tác với một môi trường (*environment*). Quá trình tương tác này có thể hiểu đơn giản như sau: tại thời điểm nhất định $\boldsymbol{t}$, tác tử lựa chọn hành động $A_t$ tương tác tới môi trường, môi trường sau khi nhận hành động này của tác tử có thể biểu hiện ra bên ngoài và bằng cơ chế quan sát (*perceive*) của mình, tác tử sẽ biết được trạng thái môi trường vừa được chuyển dịch (*transition*) sau hành động của mình là $S__\boldsymbol{t}$, cũng như phần thưởng (*reward*) $R_\boldsymbol{t}$. Ta có thể hiểu nôm na phần thưởng như một độ đo để cho tác tử biết rằng hành động nó vừa thực hiện tốt hay xấu và tốt hay xấu như thế nào. Bài toán học tăng cường được phát biểu rất đơn giản: *Làm thể nào để có thể điều khiển được một tác tử để phần thưởng mà nó thu được là lớn nhất?*

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

Thông thường, các bài toán nghiên cứu về Học tăng cường giả sử môi trường mà tác tử tương tác là môi trường ngẫu nhiên (*stochastic*) và khả năng quan sát của tác tử là đủ chi tiết để nó có thể thu thập đủ tốt dữ liệu từ môi trường.

# Lịch sử về các phương pháp giải quyết bài toán MDP

Một trong những cách hiệu quả nhất để mô hình bài toán Học tăng cường đó là sử dụng mô hình chuỗi đưa ra quyết định Markov (*Markovian Decision Process*). Mô hình này sẽ được trình bày ở bài sau. Thuở ban đầu, phương pháp được coi là chuẩn để giải các bài toán MDP là sử dụng quy hoạch động. Tuy nhiên phương pháp giải này chỉ có thể giải trong trường hợp tập trạng thái và tập hành động là rất nhỏ, đối với các bài toán mà tập không gian trạng thái, hành động lớn hơn (các bài toán thực tế trong robotics có chiều không gian hành động lên đến 12) thì quy hoạch động tỏ ra không khả thi (*infeasible*). 

Để giải MDP hiệu quả hơn, có hai ý tưởng chính như sau: Ý tưởng đầu tiên là giải quyết bài toán điều khiển (*control problem*), có thể hiểu đơn giản là: *Làm thể nào để chúng ta có thể đưa ra một quyết định hiệu quả?*; Ý tưởng tiếp theo là giải quyết bài toán dự đoán giá trị (*value prediction*) bằng các phương pháp xấp xỉ, ta có thể tạm hiểu bài toán thứ hai như là: *Làm thể nào để chúng ta có thể đánh giá được mối quan hệ giữa trạng thái, hành động và phần thưởng?*

# Một số cặp khái niệm quan trọng trong Học tăng cường
Trong học tăng cường, có rất nhiều cặp khái niệm đi cặp với nhau, một số tương phản nhau, một số thì bổ sung ý nghĩa cho nhau, các cặp khái niệm trình bày ngay trong chương này thì có thể người đọc chưa làm quen với Học tăng cường không hiểu được ngay nhưng xuyên suốt blog này, các khái niệm này sẽ được nhắc lại, vì vậy, nếu người đọc lúc này chưa quen với các khái niệm này thì có thể tạm ghi nhớ tên gọi của nó để về sau sẽ được phân tích chi tiết hơn.
## Exploitation vs. Exploration
Trong Học tăng cường, tận dụng (*exploitation*) và khai phá (*exploration*) là hai thành phần không thể thiếu của bất kỳ giải thuật học tăng cường nào. Giả sử như tác tử của chúng ta muốn đưa ra quyết định hiệu quả, thì nó phải thực hiện khai phá đủ để có thể biết được đặc trưng của các hành động. Khi đã khai phá xong và nhận biết được đặc trưng các hành động, tác tử phải tiến hành tận dưng những đặc trưng khai thác được để đưa ra hành động có phần thưởng lớn nhất.

## On-policy vs. Off-policy
Trong các bài toán điều khiển, tác tử sẽ đưa ra quyết định tuân theo một chính sách hành vi (*behavior policy*) và nó học một chính sách mục tiêu (*target policy*). Nếu như chính sách mục tiêu khác với chính sách hành vi, nó được gọi là off-policy, còn ngược lại, được coi là on-policy.

## Model-based vs. Model-free
Khái niệm này xuất hiện trong cả bài toán dự đoán và bài toán điều khiển. Nếu thuật toán mà chúng ta xây dựng học biểu diễn môi trường, thì thuật toán đó được coi là Model-based, ngược lại, nếu thuật toán không học biểu diễn môi trường, thì thuật toán được coi là Model-free.

## Value-based vs. Policy-based
Value-based là các giải thuật dựa vào học dự đoán giá trị, còn policy-based là các giải thuật dựa vào một chính sách.

# Sơ lược về các bài viết của blog
Blog này có thể hiểu sẽ được chia làm 3 phần: Phần đầu giới thiệu về các khái niệm cơ bản nhất của Học tăng cường, phần hai giới thiệu về bài toán dự đoán giá trị và các giải thuật để giải quyết bài toán này, phần cuối cùng giới thiệu về bài toán điều khiển cũng như các giải thuật liên quan.
