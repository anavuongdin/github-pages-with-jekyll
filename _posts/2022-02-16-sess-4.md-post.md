---
title: "Bài 4: Giới thiệu về bài toán dự đoán giá trị (Value prediction)"
date: 2022-02-16
image: /assets/img/reinforcement-learning.webp
---
Như vậy, qua 3 bài viết đầu tiên, những khái niệm cơ bản về học tăng cường đã được trình bày, ngoài ra, một bài toán ví dụ mở đầu về học tăng cường cũng đã được nêu và bắt đầu từ bài viết này, ta sẽ xem xét vào những khía cạnh sâu hơn của học tăng cường. Bài viết này sẽ giới thiệu về bài toán dự đoán giá trị (*value prediction problems*).

## Phép biến đổi Bellman
Nhắc lại khái niệm của MRP: Khi nói đến MRP là ta đang xem xét MDP gán với một chính sách cụ thể $\pi$ nào đó. Và bài toán dự đoán giá trị rất đơn giản, đó là chúng ta cần phải ước lượng được hàm $V^{\pi}$. Xét trường hợp chính sách $\pi$ cố định. Ta gọi $T^{\pi}: \mathbb{R}^{\mathcal{X}}\rightarrow\mathbb{R}^{\mathcal{X}}$ là phép biến đổi Bellman gán với chính sách $\pi$, cụ thể như sau:

$$ (T^{\pi}V)(x) = r(x, \pi(x)) + \gamma\times\sum_{y\in\mathcal{X}}\mathcal{P}(y,R \| x, \pi(x))\times V(y), \forall x\in\mathcal{X}$$

Phương trình Bellman ở bài 2 đã cho ta biết rằng: $V^{\pi}(x) = r(x, \pi(x)) + \gamma\times\sum_{y\in\mathcal{X}}\mathcal{P}(y,R \| x, \pi(x))\times V^{\pi}(y), \forall x\in\mathcal{X}$, do đó, nói cách khác, ta phải có đẳng thức sau:

$$ T^{\pi}V^{\pi} = V^{\pi}$$

Một cách tương tự, ta định nghĩa phép biến đổi Bellman tối ưu như sau:

$$ (T^{\ast}V)(x) = \sup_{a\in\mathcal{A}} \{r(x, \pi(x)) + \gamma\times\sum_{y\in\mathcal{X}}\mathcal{P}(y,R \| x, \pi(x))\times V^{\pi}(y), \} \forall x\in\mathcal{X}$$

Và ta cũng có từ kết quả bài 2 rằng: $T^{\ast}V^{\ast}=V^{\ast}$. Ta sẽ đi phân tích ý nghĩa của $T^{\pi}$ và $T^{\ast}$. Dựa vào $T^{\pi}$, ta có thể cài đặt các thuật toán quy hoạch động để tính chính xác được $V^{\pi}$, qua đó, ta sẽ biết được rằng giá trị ứng với chính sách $\pi$ có tốt hay không. Còn đối với $T^{\ast}$, rõ ràng, chúng ta không thể "quy hoạch động" để tìm $V^{\ast}$ dựa vào $V^{\ast}$, tuy nhiên, người ta đã chứng minh được lý thuyết rằng việc áp dụng phép biến đổi tối ưu Bellman sẽ hội tụ về được $V^{\ast}$ dưới một số điều kiện nhát định - phần tiếp theo sẽ trình bày kỹ hơn về phần này. Vậy, ta có thể hiểu rằng $T^{\ast}$ được ứng dụng để tìm giá hàm giá trị tại chính sách tối ưu. Trước khi trình bày phần tiếp theo, mục này sẽ khép lại bằng mối quan hệ của $T, V$ với $Q$, ta có:

$$ T^{\pi}Q(x, a) = r(x, a) + \gamma\times\sum_{y\in\mathcal{X}}\mathcal{P}(y,R \| x, a)\times Q(y, \pi(y)), \forall x, a\in\mathcal{X}\times\mathcal{A}$$

$$ T^{\ast}Q(x, a) = r(x, a) + \gamma\times\sum_{y\in\mathcal{X}}\mathcal{P}(y,R \| x, a)\times \sup_{b\in\mathcal{A}} Q(y, b), \forall x, a\in\mathcal{X}\times\mathcal{A}$$

## Phương pháp value iteration

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
