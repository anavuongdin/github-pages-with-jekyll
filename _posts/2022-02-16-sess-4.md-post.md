---
title: "Bài 4: Giới thiệu về bài toán dự đoán giá trị (Value prediction)"
date: 2022-02-16
image: /assets/img/post4/value-iteration.png
---
Như vậy, qua 3 bài viết đầu tiên, những khái niệm cơ bản về học tăng cường đã được trình bày, ngoài ra, một bài toán ví dụ mở đầu về học tăng cường cũng đã được nêu và bắt đầu từ bài viết này, ta sẽ xem xét vào những khía cạnh sâu hơn của học tăng cường. Bài viết này sẽ giới thiệu về bài toán dự đoán giá trị (*value prediction problems*). Nhắc lại khái niệm của MRP: Khi nói đến MRP là ta đang xem xét MDP gán với một chính sách cụ thể $\pi$ nào đó. Và bài toán dự đoán giá trị rất đơn giản, đó là chúng ta cần phải ước lượng được hàm $V^{\pi}$. 

## Phép biến đổi Bellman
Xét trường hợp chính sách $\pi$ cố định. Ta gọi $T^{\pi}: \mathbb{R}^{\mathcal{X}}\rightarrow\mathbb{R}^{\mathcal{X}}$ là phép biến đổi Bellman gán với chính sách $\pi$, cụ thể như sau:

$$ (T^{\pi}V)(x) = r(x, \pi(x)) + \gamma\times\sum_{y\in\mathcal{X}}\mathcal{P}(y,R \| x, \pi(x))\times V(y), \forall x\in\mathcal{X}$$

Phương trình Bellman ở bài 2 đã cho ta biết rằng: $V^{\pi}(x) = r(x, \pi(x)) + \gamma\times\sum_{y\in\mathcal{X}}\mathcal{P}(y,R \| x, \pi(x))\times V^{\pi}(y), \forall x\in\mathcal{X}$, do đó, nói cách khác, ta phải có đẳng thức sau:

$$ T^{\pi}V^{\pi} = V^{\pi}$$

Một cách tương tự, ta định nghĩa phép biến đổi Bellman tối ưu như sau:

$$ (T^{\ast}V)(x) = \sup_{a\in\mathcal{A}} \{r(x, \pi(x)) + \gamma\times\sum_{y\in\mathcal{X}}\mathcal{P}(y,R \| x, \pi(x))\times V^{\pi}(y), \} \forall x\in\mathcal{X}$$

Và ta cũng có từ kết quả bài 2 rằng: $T^{\ast}V^{\ast}=V^{\ast}$. Ta sẽ đi phân tích ý nghĩa của $T^{\pi}$ và $T^{\ast}$: 
- Dựa vào $T^{\pi}$, ta có thể cài đặt các thuật toán quy hoạch động để tính chính xác được $V^{\pi}$, qua đó, ta sẽ biết được rằng giá trị ứng với chính sách $\pi$ có tốt hay không. 
- Còn đối với $T^{\ast}$, rõ ràng, chúng ta không thể "quy hoạch động" để tìm $V^{\ast}$ dựa vào $V^{\ast}$, tuy nhiên, người ta đã chứng minh được lý thuyết rằng việc áp dụng phép biến đổi tối ưu Bellman sẽ hội tụ về được $V^{\ast}$ dưới một số điều kiện nhất định - phần tiếp theo sẽ trình bày kỹ hơn về phần này. Vậy, ta có thể hiểu rằng $T^{\ast}$ được ứng dụng để tìm giá hàm giá trị tại chính sách tối ưu. 

Trước khi trình bày phần tiếp theo, mục này sẽ khép lại bằng mối quan hệ của $T, V$ với $Q$, ta có:

$$ T^{\pi}Q(x, a) = r(x, a) + \gamma\times\sum_{y\in\mathcal{X}}\mathcal{P}(y,R \| x, a)\times Q(y, \pi(y)), \forall x, a\in\mathcal{X}\times\mathcal{A}$$

$$ T^{\ast}Q(x, a) = r(x, a) + \gamma\times\sum_{y\in\mathcal{X}}\mathcal{P}(y,R \| x, a)\times \sup_{b\in\mathcal{A}} Q(y, b), \forall x, a\in\mathcal{X}\times\mathcal{A}$$

## Phương pháp value iteration (quy hoạch động xấp xỉ)
Xét dãy hàm giá trị khởi đầu bằng giá trị của $V_0$ bất kỳ, các phần tử tiếp theo được tính bằng công thức truy hồi như sau:

$$ V_{k+1} = T^{\ast} V_k, \forall k\geq 0 $$

Định lý *Banach fixed-point* chứng minh được dãy hàm này sẽ hội tụ về $V^{\ast}$. Một cách tương tự, bằng cách xét dãy hàm:

$$ Q_{k+1} = T^{\ast} Q_k, \forall k\geq 0, (1) $$

Ta cũng sẽ thu được dãy hàm hội tụ đến giá trị $Q^{\ast}$. Như vậy, có thể thấy rằng các phương pháp này cho ta xấp xỉ được các giá trị tối ưu $V^{\ast}, Q^{\ast}$, bởi lẽ đó, người ta còn hay gọi các phương pháp này là quy hoạch động xấp xỉ.

Bên cạnh ý nghĩa tìm ra giá trị tối ưu tại từng trạng thái, phương pháp value iteration này còn là tiền đề cho một số phương pháp Policy Iteration. Ta gọi $\pi$ là chính sách tham lam tương ứng với $Q$, khi đó, ta có kết quả của một chứng minh bởi Singh et al. như sau:

$$ V^{\pi}(x) \geq V^{\ast}(x)-\dfrac{2}{1-\gamma} \| Q^{\pi}-Q^{\ast} \|, \forall x\in\mathcal{X} $$

Trong đó, chuẩn được sử dụng ở trên là chuẩn L1. Định lý này cho ta biết rằng, nếu ta càng làm giảm được $Q^{\pi}$ về giá trị tối ưu $Q^{\ast}$ thì chính sách của ta thu được làm cho hàm giá trị càng gần được về tối ưu. Chính vì vậy, ở trong bước cập nhật $(1)$, nếu ta gọi $\pi_k$ là chính sách tham lam tương ứng với $Q_k$ cập nhật theo từng bước thì từ định lý trên ta sẽ có $V^{\pi_k}(x)$ hội tụ về $V^{\ast}$. Bài toán dự đoán giá trị không giải quyết việc đưa ra cơ chế lựa chọn hành động mà chỉ đi ước lượng giá trị, do đó, muốn giải được các bài toán học tăng cường, cần phải định nghĩa thêm về các cơ chế đưa ra hành động. Bài toán này được gọi là bài toán điều khiển, sẽ được trình bày ở các bài sau, còn hiện tại, ta tiếp tục đi sâu vào phân tích bài toán dự đoán giá trị.

Thuật toán Value Iteration được trình bày ngắn gọn như sau:
\begin{algorithmic}
\State $i \gets 10$
\If{$i\geq 5$} 
    \State $i \gets i-1$
\Else
    \If{$i\leq 3$}
        \State $i \gets i+2$
    \EndIf
\EndIf 
\end{algorithmic}


<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'] ],
        processEscapes: true
      }
    });
  </script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
