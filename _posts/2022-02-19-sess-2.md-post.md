---
title: "Bài 2: Chuỗi đưa quyết định Markov (MDP)"
date: 2022-02-19
image: /assets/img/reinforcement-learning.webp
---

Bây giờ Chúng ta hãy chính thức xác định một tập hợp các khái niệm chính trong RL.

Tác nhân đang hoạt động trong một môi trường. Cách môi trường phản ứng với các hành động nhất định được xác định bởi một mô hình mà chúng ta có thể biết hoặc có thể không biết. Tác nhân có thể ở một trong nhiều trạng thái ($$ s \ in \ mathcal {S} $$) của môi trường và chọn thực hiện một trong nhiều hành động ($$ a \ in \ mathcal {A} $$) để chuyển từ trạng thái này sang trạng thái khác. Người đại diện sẽ đến bang nào được quyết định bởi xác suất chuyển đổi giữa các bang ($$ P $$). Sau khi một hành động được thực hiện, môi trường sẽ gửi phần thưởng ($$ r \ in \ mathcal {R} $$) dưới dạng phản hồi.

Mô hình xác định hàm phần thưởng và xác suất chuyển đổi. Chúng tôi có thể biết hoặc không biết cách hoạt động của mô hình và điều này phân biệt hai trường hợp:

Biết mô hình: lập kế hoạch với thông tin hoàn hảo; làm RL dựa trên mô hình. Khi hiểu biết đầy đủ về môi trường, chúng ta có thể tìm ra giải pháp tối ưu bằng Lập trình động (DP). Bạn có còn nhớ "sự gia tăng con dài nhất" hoặc "vấn đề người bán hàng lưu động" trong lớp Algorithm 101 của bạn không? CƯỜI. Đây không phải là trọng tâm của bài đăng này.
Không biết mô hình: học với thông tin không đầy đủ; thực hiện RL không có mô hình hoặc cố gắng tìm hiểu mô hình một cách rõ ràng như một phần của thuật toán. Hầu hết nội dung sau đây phục vụ các tình huống khi mô hình không xác định.
Chính sách của đại lý $$ \ pi (s) $$ cung cấp hướng dẫn về hành động tối ưu cần thực hiện ở một trạng thái nhất định với mục tiêu tối đa hóa tổng số phần thưởng. Mỗi trạng thái được liên kết với một hàm giá trị $$ V (s) $$ dự đoán số lượng phần thưởng dự kiến ​​trong tương lai mà chúng tôi có thể nhận được ở trạng thái này bằng cách thực hiện chính sách tương ứng. Nói cách khác, hàm giá trị định lượng trạng thái tốt như thế nào. Cả hai chức năng chính sách và giá trị đều là những gì chúng tôi cố gắng tìm hiểu trong quá trình học tăng cường.

! [Phân loại các thuật toán RL] ({{'/assets/images/RL_algorithm_categorization.png' | relative_url}}) {: style = "width: 100%;" class = "center"} Hình 2. Tóm tắt các phương pháp tiếp cận trong RL dựa trên việc chúng ta muốn mô hình hóa giá trị, chính sách hay môi trường. (Nguồn hình ảnh: sao chép từ bài giảng khóa học RL của David Silver 1.)

Sự tương tác giữa tác nhân và môi trường liên quan đến một chuỗi hành động và phần thưởng quan sát được trong thời gian, $$ t = 1, 2, \ dot, T $$. Trong quá trình này, đại lý tích lũy kiến ​​thức về môi trường, tìm hiểu chính sách tối ưu và đưa ra quyết định thực hiện hành động nào tiếp theo để tìm hiểu hiệu quả chính sách tốt nhất. Hãy gắn nhãn trạng thái, hành động và phần thưởng tại thời điểm bước t là $$ S_t $$, $$ A_t $$ và $$ R_t $$, tương ứng. Do đó, trình tự tương tác được mô tả đầy đủ bằng một tập (còn được gọi là "thử nghiệm" hoặc "quỹ đạo") và trình tự kết thúc ở trạng thái đầu cuối $$ S_T $$:

$$ S_1, A_1, R_2, S_2, A_2, \ dot, S_T $$

Các thuật ngữ bạn sẽ gặp phải rất nhiều khi đi sâu vào các danh mục khác nhau của thuật toán RL:

Dựa trên mô hình: Dựa trên mô hình của môi trường; hoặc mô hình đã biết hoặc thuật toán học nó một cách rõ ràng.
Không có mô hình: Không phụ thuộc vào mô hình trong quá trình học.
Đúng chính sách: Sử dụng kết quả xác định hoặc mẫu từ chính sách mục tiêu để đào tạo thuật toán.
Ngoài chính sách: Đào tạo về cách phân phối các chuyển đổi hoặc các tập được tạo ra bởi một chính sách hành vi khác chứ không phải do chính sách mục tiêu tạo ra. 


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
