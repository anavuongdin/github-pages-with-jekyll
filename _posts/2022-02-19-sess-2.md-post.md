---
title: "Bài 2: Chuỗi đưa quyết định Markov (MDP)"
date: 2022-02-19
image: /assets/img/reinforcement-learning.webp
---

Trong bài viết thứ hai này, mình sẽ giới thiệu về Chuỗi đưa quyết định Markov (*Markov decision process*). Phần đầu tiên sẽ trình bày về một số kiến thức liên quan như Đại số tuyến tính, Giải tích, Xác suất, ... Sau đó, phần thứ hai trình bày về các khái niệm cơ bản trong MDP và mình kết thúc bài viết bằng một ví dụ.

## Sơ khảo các kiến thức liên quan
- Như thường lệ, ta gọi $\mathbb{N}=\{0,1,2\dots\}$ là tập số tự nhiên, $\mathbb{R}$ là tập số thực. 

- Một vector $u\in\mathbb{R}^d$ là đại lượng có hướng, được biểu diễn bởi $(u_1, u_2, \dots, u_d)$, mỗi $u_i, 1\leq i\leq d$ thường được gọi là một *thành phần* (feature). Tích vô hướng giữa hai vector $u, v\in\mathbb{R}^d$ là $\langle u, v\rangle = \sum_{i=1}^d u_iv_i$.

- Một số các chuẩn trong không gian metric quen thuộc như: L2: $\| u\| = \langle u, u\rangle$, L1: $\| u\| = \max_{i=1,2,\dots, d}\|u_i\|$.

- Đối với một hàm $v$ phụ thuộc vào $\theta$, giả sử: $v = v(\theta, x)$. Khi đó, đạo hàm riêng của $v$ theo $\theta$ được ký hiệu là: $\frac{\partial v}{\partial \theta}$. Đạo hàm toàn phần của $v$ theo $\theta$ được ký hiệu là $\nabla_{\theta} v = \frac{dv}{d\theta}$. 

- Giả sử $P$ là một phân phối xác suất nào đó. Nếu biến ngẫu nhiên $X$ được sinh ra bằng cách chọn mẫu dựa trên phân phối $P$, ta viết là: $X{\sim}P$.

## Chuỗi đưa quyết định Markov
Một chuỗi Markov thường được biểu diễn bởi bộ ba $\mathcal{M} = (\mathcal{X}, \mathcal{A}, \mathcal{P}_{0})$. Trong đó $\mathcal{X}$ là tập không gian trạng thái, $\mathcal{A}$ là tập không gian hành động. Phân phối xác suất $\mathcal{P}_{0}$ được gọi là phân phối xác suất chuyển trạng thái. Một MDP được gọi là hữu hạn nếu cả $\mathcal{X}$ và $\mathcal{A}$ đều hữu hạn.

Giả sử, trạng thái hiện tại của môi trường là $x$, tác tử của chúng ta chọn một hành động là $a$. Khi đó: $\mathcal{P}_{0}(y, R\|x, a)$ sẽ cho ta biết xác suất mà trạng thái của môi trường là $y$ và phần thưởng mà tác tử nhận được trong sự chuyển dịch này là $R$. Ta có thể thấy rằng: $\mathcal{P}_{0}(\cdot\|x, a)$ là một ánh xạ: $\mathcal{X}\times\mathbb{R}\rightarrow\mathbb{R}$. Thông thường, phân phối xác suất chuyển trạng thái là ẩn với tác tử, nghĩa là tác tử không thể nào biết được phân phối xác suất trạng thái môi trường nếu nó thực hiện hành động $a$ tại trạng thái $x$. Trong bài toán điều khiển, nếu tác tử cố gắng học biểu diễn phân phối xác suất này bằng một phương pháp xấp xỉ nào đó, ta gọi đó là model-based, ngược lại, nếu tác tử không dựa vào học biểu diễn này, thì ta gọi đó là model-free. Các phương pháp dựa trên model-free thường chạy nhanh hơn, tuy nhiên, độ chính xác không bằng được model-based. Ta sẽ xem xét chi tiết hơn về phần này ở các bài sau.

Phần thưởng chính là một đại lượng để ước lượng xem rằng hành động vừa chọn có tốt hay không và tốt hay không ra sao. Phần thưởng càng lớn thì hành động vừa đưa ra càng tốt, càng nhỏ thì hành động đưa ra càng tệ. Như đã trình bày ở phần trước, ta có: $(y, R){\sim}\mathcal{P}_{0}(y, R|x, a)$. Mỗi giá trị $R_{(x,a)}$ được sinh ra ở từng bước chuyển vị được gọi là *phần thưởng tức khắc* (*immediate reward*). Ta định nghĩa thêm đại lượng quan trọng sau:
$$ r(x, a) = \EX(R_{(x, a)})$$

Bây giờ, ta sẽ xem xét chuỗi Markov theo thời gian, lý do bởi sự tương tác giữa tác tử với môi trường diễn ra theo chương, hồi. Ta gọi $t$ là bước thời gian hiện tại của chuỗi, gọi $X_t\in\mathbb{X}$ là trạng thái hiện tại của môi trường, $A_t\in\mathbb{A}$ là hành động được lựa chọn.

{:refdef: style="text-align: center;"}
  ![Minh họa về Học tăng cường]({{ page.image | relative_url }}){: .center-image }  
  *Hình 1: Minh họa về Học tăng cường* 
{: refdef}

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
